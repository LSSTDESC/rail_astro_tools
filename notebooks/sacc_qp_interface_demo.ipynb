{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QP/SACC Interface Demo\n",
        "\n",
        "This notebook demonstrates the usage of the `QPToSACC` and `SACCToQP` RailStage classes that provide functionality to move n(z) distributions between qp and sacc file formats.\n",
        "\n",
        "The demo uses the same gold-baseline data as the `gold_baseline_tutorial.ipynb` to ensure compatibility and demonstrate the conversion workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goals\n",
        "\n",
        "- Demonstrate converting n(z) distributions from qp format to sacc format using `QPToSACC`\n",
        "- Demonstrate converting n(z) distributions from sacc format back to qp format using `SACCToQP`\n",
        "- Validate that the round-trip conversion preserves the data correctly\n",
        "- Use the same gold-baseline test data as the tutorial notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import qp\n",
        "import sacc\n",
        "import nz_prior as nzp\n",
        "\n",
        "from rail.core.stage import RailStage\n",
        "from rail.tools.sacc_tools import (\n",
        "    QPToSACC,\n",
        "    SACCToQP,\n",
        "    normalize_hist,\n",
        "    extract_tomographic_bins_from_sacc,\n",
        ")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "plt.rcParams[\"axes.labelsize\"] = 14\n",
        "plt.rcParams[\"axes.titlesize\"] = 16\n",
        "plt.rcParams[\"xtick.labelsize\"] = 13\n",
        "plt.rcParams[\"ytick.labelsize\"] = 13\n",
        "plt.rcParams[\"legend.fontsize\"] = 13\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data\n",
        "\n",
        "We'll use the gold-baseline data from the NZ Data Challenge. If it's not present, we'll download it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_ROOT = Path(\"./gold_baseline\")\n",
        "ALGORITHM = \"tpz\"\n",
        "N_BINS = 5\n",
        "OUTPUT_DIR = Path(\"./output_sacc_demo\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "SAVE_ROOT = OUTPUT_DIR / \"priors\"\n",
        "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download data if needed\n",
        "if not DATA_ROOT.exists():\n",
        "    url = \"https://s3df.slac.stanford.edu/people/echarles/package_test_data/nz_prior/gold_baseline.tgz\"\n",
        "    tgz_path = Path(\"gold_baseline.tgz\")\n",
        "    \n",
        "    print(f\"'{DATA_ROOT}' directory not found. Downloading test data from {url} ...\")\n",
        "    urllib.request.urlretrieve(url, tgz_path)\n",
        "    print(\"Download complete. Extracting...\")\n",
        "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "    print(\"Extraction complete.\")\n",
        "    tgz_path.unlink()\n",
        "    \n",
        "    if not DATA_ROOT.exists():\n",
        "        raise FileNotFoundError(f\"Failed to download and extract '{DATA_ROOT}' from {url}.\")\n",
        "\n",
        "print(f\"Data directory: {DATA_ROOT}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Input Files\n",
        "\n",
        "First, we'll identify the qp files for each tomographic bin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare lists of input qp files and truth files\n",
        "qp_files = []\n",
        "truth_files = []\n",
        "tracer_names = []\n",
        "\n",
        "for bin_index in range(N_BINS):\n",
        "    tracer_name = f\"bin_{bin_index}\"\n",
        "    tracer_names.append(tracer_name)\n",
        "    \n",
        "    # Ensemble file\n",
        "    ensemble_path = DATA_ROOT / f\"output_summarize_{ALGORITHM}_uniform_binning_bin{bin_index}_naive_stack.hdf5\"\n",
        "    qp_files.append(str(ensemble_path))\n",
        "    \n",
        "    # Truth file\n",
        "    truth_path = DATA_ROOT / f\"true_NZ_true_nz_{ALGORITHM}_uniform_binning_bin{bin_index}.hdf5\"\n",
        "    truth_files.append(str(truth_path))\n",
        "\n",
        "print(f\"Found {len(qp_files)} qp files:\")\n",
        "for name, qp_file in zip(tracer_names, qp_files):\n",
        "    print(f\"  {name}: {Path(qp_file).name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert QP to SACC\n",
        "\n",
        "Now we'll use the `QPToSACC` class to convert the qp files into a sacc catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the RailStage data store\n",
        "RailStage.data_store.clear()\n",
        "DS = RailStage.data_store\n",
        "\n",
        "# Create QPToSACC stage\n",
        "qp_to_sacc = QPToSACC.make_stage(\n",
        "    name='qp_to_sacc',\n",
        "    tracer_names=tracer_names,\n",
        "    truth_files=truth_files if all(os.path.exists(f) for f in truth_files) else None,\n",
        ")\n",
        "\n",
        "# Run the conversion\n",
        "sacc_output_path = str(OUTPUT_DIR / \"nz_sacc_catalog.fits\")\n",
        "sacc_handle = qp_to_sacc(qp_files)\n",
        "\n",
        "# Save the sacc catalog to file\n",
        "sacc_catalog = sacc_handle.data\n",
        "sacc_catalog.save_fits(sacc_output_path, overwrite=True)\n",
        "\n",
        "print(f\"Created sacc catalog with {len(sacc_catalog.tracers)} tracers:\")\n",
        "for tracer_name in sacc_catalog.tracers.keys():\n",
        "    tracer = sacc_catalog.tracers[tracer_name]\n",
        "    print(f\"  {tracer_name}: {tracer.tracer_type}\")\n",
        "print(f\"\\nSaved to: {sacc_output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert SACC back to QP\n",
        "\n",
        "Now we'll use the `SACCToQP` class to convert the sacc catalog back into individual qp files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SACCToQP stage\n",
        "sacc_to_qp = SACCToQP.make_stage(\n",
        "    name='sacc_to_qp',\n",
        "    output_dir=str(OUTPUT_DIR / \"qp_output\"),\n",
        "    output_prefix=\"reconverted\",\n",
        "    tracer_names=None,  # Extract all tracers\n",
        "    include_truth=True,\n",
        ")\n",
        "\n",
        "# Run the conversion\n",
        "qp_output_handle = sacc_to_qp(sacc_output_path)\n",
        "\n",
        "# Get the output file paths\n",
        "if isinstance(qp_output_handle.data, dict):\n",
        "    qp_output_files = qp_output_handle.data\n",
        "    print(f\"Created {len(qp_output_files)} qp files:\")\n",
        "    for tracer_name, file_path in qp_output_files.items():\n",
        "        print(f\"  {tracer_name}: {Path(file_path).name}\")\n",
        "else:\n",
        "    qp_output_files = {list(sacc_catalog.tracers.keys())[0]: qp_output_handle.data}\n",
        "    print(f\"Created qp file: {Path(qp_output_files[list(qp_output_files.keys())[0]]).name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation: Compare Original and Reconstructed Data\n",
        "\n",
        "Let's compare the original qp files with the reconstructed ones to verify the round-trip conversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to extract PDFs and edges from qp ensemble (handles both formats)\n",
        "def extract_pdfs_and_edges(ensemble):\n",
        "    \"\"\"Extract PDFs and bin edges from qp ensemble, handling both histogram and interpolated formats.\"\"\"\n",
        "    # Check for histogram format first\n",
        "    has_pdfs_objdata = hasattr(ensemble, \"objdata\") and \"pdfs\" in ensemble.objdata\n",
        "    has_bins_objdata = hasattr(ensemble, \"objdata\") and \"bins\" in ensemble.objdata\n",
        "    has_bins_metadata = hasattr(ensemble, \"metadata\") and \"bins\" in ensemble.metadata\n",
        "    has_xvals_metadata = hasattr(ensemble, \"metadata\") and \"xvals\" in ensemble.metadata\n",
        "    has_yvals_objdata = hasattr(ensemble, \"objdata\") and \"yvals\" in ensemble.objdata\n",
        "    \n",
        "    if has_pdfs_objdata:\n",
        "        # Histogram format\n",
        "        pdfs = np.array(ensemble.objdata[\"pdfs\"], dtype=float)\n",
        "        if pdfs.ndim == 1:\n",
        "            pdfs = pdfs.reshape(1, -1)\n",
        "        if has_bins_objdata:\n",
        "            edges = np.array(ensemble.objdata[\"bins\"], dtype=float).flatten()\n",
        "        elif has_bins_metadata:\n",
        "            edges = np.array(ensemble.metadata[\"bins\"], dtype=float).flatten()\n",
        "        else:\n",
        "            raise ValueError(\"Could not find bin edges for histogram format\")\n",
        "    elif has_xvals_metadata and has_yvals_objdata:\n",
        "        # Interpolated format - convert to histogram\n",
        "        xvals = np.array(ensemble.metadata[\"xvals\"], dtype=float).flatten()\n",
        "        yvals = np.array(ensemble.objdata[\"yvals\"], dtype=float)\n",
        "        # Handle NaN/inf values\n",
        "        yvals = np.nan_to_num(yvals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        if yvals.ndim == 1:\n",
        "            yvals = yvals.reshape(1, -1)\n",
        "        # Convert to bin values by averaging adjacent grid points\n",
        "        pdfs = 0.5 * (yvals[:, :-1] + yvals[:, 1:])\n",
        "        edges = xvals\n",
        "    else:\n",
        "        raise ValueError(\"Could not extract PDFs from ensemble - unknown format\")\n",
        "    \n",
        "    return pdfs, edges\n",
        "\n",
        "# Compare all tracers\n",
        "fig, axes = plt.subplots(2, N_BINS, figsize=(N_BINS * 3.5, 8))\n",
        "if N_BINS == 1:\n",
        "    axes = axes.reshape(2, 1)\n",
        "\n",
        "for idx, tracer_name in enumerate(tracer_names):\n",
        "        \n",
        "    # Load original qp file\n",
        "    orig_ensemble = qp.read(qp_files[idx])\n",
        "    \n",
        "    # Load reconstructed qp file\n",
        "    if isinstance(qp_output_files, dict):\n",
        "        recon_path = qp_output_files[tracer_name]\n",
        "    else:\n",
        "        recon_path = qp_output_files[list(qp_output_files.keys())[0]]\n",
        "    recon_ensemble = qp.read(recon_path)\n",
        "    \n",
        "    # Extract PDFs and edges using helper function\n",
        "    try:\n",
        "        orig_pdfs, orig_edges = extract_pdfs_and_edges(orig_ensemble)\n",
        "        recon_pdfs, recon_edges = extract_pdfs_and_edges(recon_ensemble)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {tracer_name}: {e}\")\n",
        "        continue\n",
        "    \n",
        "    # Calculate centers\n",
        "    orig_z = 0.5 * (orig_edges[:-1] + orig_edges[1:])\n",
        "    recon_z = 0.5 * (recon_edges[:-1] + recon_edges[1:])\n",
        "    \n",
        "    # Plot mean PDFs\n",
        "    ax = axes[0, idx]\n",
        "    orig_mean = np.mean(orig_pdfs, axis=0) if orig_pdfs.ndim > 1 else orig_pdfs\n",
        "    recon_mean = np.mean(recon_pdfs, axis=0) if recon_pdfs.ndim > 1 else recon_pdfs\n",
        "    \n",
        "    ax.plot(orig_z, orig_mean, label=\"Original\", alpha=0.7, linewidth=2)\n",
        "    ax.plot(recon_z, recon_mean, label=\"Reconstructed\", linestyle=\"--\", alpha=0.7, linewidth=2)\n",
        "    ax.set_xlabel(\"z\")\n",
        "    ax.set_ylabel(\"n(z)\")\n",
        "    ax.set_title(f\"{tracer_name}\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot difference\n",
        "    ax = axes[1, idx]\n",
        "    # Interpolate to common z grid for comparison\n",
        "    z_common = np.linspace(max(orig_z.min(), recon_z.min()), \n",
        "                          min(orig_z.max(), recon_z.max()), 100)\n",
        "    orig_interp = np.interp(z_common, orig_z, orig_mean, left=0, right=0)\n",
        "    recon_interp = np.interp(z_common, recon_z, recon_mean, left=0, right=0)\n",
        "    diff = orig_interp - recon_interp\n",
        "    \n",
        "    ax.plot(z_common, diff, linewidth=2)\n",
        "    ax.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
        "    ax.set_xlabel(\"z\")\n",
        "    ax.set_ylabel(\"Difference\")\n",
        "    ax.set_title(f\"{tracer_name} (Diff)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify SACC Catalog Structure\n",
        "\n",
        "Let's inspect the sacc catalog to see what tracers were created.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the sacc catalog to verify\n",
        "loaded_sacc = sacc.Sacc.load_fits(sacc_output_path)\n",
        "\n",
        "print(f\"SACC catalog contains {len(loaded_sacc.tracers)} tracers:\\n\")\n",
        "\n",
        "for tracer_name, tracer in loaded_sacc.tracers.items():\n",
        "    print(f\"Tracer: {tracer_name}\")\n",
        "    print(f\"  Type: {tracer.tracer_type}\")\n",
        "    if hasattr(tracer, 'ensemble') and tracer.ensemble is not None:\n",
        "        ensemble = tracer.ensemble\n",
        "        if hasattr(ensemble, 'objdata') and 'pdfs' in ensemble.objdata:\n",
        "            pdfs = ensemble.objdata['pdfs']\n",
        "            print(f\"  Ensemble size: {pdfs.shape[0]} realizations\")\n",
        "            print(f\"  PDF bins: {pdfs.shape[1] if pdfs.ndim > 1 else len(pdfs)}\")\n",
        "    if hasattr(tracer, 'z') and tracer.z is not None:\n",
        "        z = tracer.z\n",
        "        print(f\"  z range: [{z.min():.3f}, {z.max():.3f}]\")\n",
        "    if hasattr(tracer, 'nz') and tracer.nz is not None:\n",
        "        nz = tracer.nz\n",
        "        print(f\"  nz range: [{nz.min():.6f}, {nz.max():.6f}]\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Uncertainties with PriorSacc Models\n",
        "\n",
        "We now apply the `nz_prior` models from the gold_baseline_tutorial to each tracer in the sacc catalog. This demonstrates how the converted sacc catalog can be used with the PriorSacc machinery to compute n(z) priors using various models (Shifts, Shifts & Widths, GP, PCA).\n",
        "\n",
        "### Helper Functions\n",
        "\n",
        "First, we need to set up helper functions and data structures similar to the gold_baseline_tutorial.\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **QPToSACC**: Successfully converted multiple qp ensemble files (one per tomographic bin) into a single sacc catalog with QPNZ tracers\n",
        "2. **SACCToQP**: Successfully converted the sacc catalog back into individual qp files\n",
        "3. **Round-trip validation**: Verified that the converted data matches the original within acceptable numerical precision\n",
        "\n",
        "The `QPToSACC` and `SACCToQP` classes provide a clean interface for moving n(z) distributions between these two formats, enabling interoperability between different analysis pipelines and tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tomographic bins from the sacc catalog using module utility\n",
        "# Note: normalize_hist and extract_tomographic_bins_from_sacc are now imported from rail.tools.sacc_tools\n",
        "tomographic_bins, edges_lookup = extract_tomographic_bins_from_sacc(sacc_catalog)\n",
        "print(f\"Loaded {len(tomographic_bins)} tomographic bins from sacc catalog\")\n",
        "for tomo in tomographic_bins:\n",
        "    print(f\"  {tomo['label']}: {tomo['estimates'].shape[0]} ensemble members, {len(tomo['z'])} z bins\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for PriorSacc models\n",
        "def sacc_from_draws(draws: Dict[str, List[np.ndarray]], edges_lookup: Dict[str, np.ndarray]) -> sacc.Sacc:\n",
        "    \"\"\"Create a sacc catalog from draw dictionaries.\"\"\"\n",
        "    catalog = sacc.Sacc()\n",
        "    for tracer_name, (_, pdfs) in draws.items():\n",
        "        edges = edges_lookup[tracer_name]\n",
        "        z_centers = 0.5 * (edges[:-1] + edges[1:])\n",
        "        pdfs = normalize_hist(pdfs, edges)\n",
        "        ensemble = qp.Ensemble(qp.hist, data={\"bins\": edges, \"pdfs\": pdfs})\n",
        "        nz_mean = pdfs.mean(axis=0)\n",
        "        catalog.add_tracer(\"QPNZ\", tracer_name, ensemble, z=z_centers, nz=nz_mean)\n",
        "    return catalog\n",
        "\n",
        "\n",
        "def tomos_from_draws(draws: Dict[str, List[np.ndarray]], edges_lookup: Dict[str, np.ndarray]) -> List[Dict[str, np.ndarray]]:\n",
        "    \"\"\"Convert draws dictionary to tomographic bins format.\"\"\"\n",
        "    tomos = []\n",
        "    for tracer_name, (_, pdfs) in draws.items():\n",
        "        edges = edges_lookup[tracer_name]\n",
        "        z_centers = 0.5 * (edges[:-1] + edges[1:])\n",
        "        pdfs = normalize_hist(pdfs, edges)\n",
        "        nz_mean = pdfs.mean(axis=0)\n",
        "        tomos.append({\n",
        "            \"label\": tracer_name,\n",
        "            \"z\": z_centers,\n",
        "            \"edges\": edges,\n",
        "            \"truth\": nz_mean,\n",
        "            \"estimates\": pdfs,\n",
        "        })\n",
        "    return tomos\n",
        "\n",
        "\n",
        "def make_nz_plots(tomos: List[Dict[str, np.ndarray]], max_samples: int = 50) -> None:\n",
        "    \"\"\"Plot n(z) distributions for all tomographic bins.\"\"\"\n",
        "    n = len(tomos)\n",
        "    fig, axes = plt.subplots(2, n, figsize=(n * 3.6, 7), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = axes.reshape(2, 1)\n",
        "    fig.subplots_adjust(wspace=0.05, hspace=0.12)\n",
        "\n",
        "    for i, tomo in enumerate(tomos):\n",
        "        z = tomo[\"z\"]\n",
        "        est = tomo[\"estimates\"]\n",
        "        truth = tomo[\"truth\"]\n",
        "        mean_est = est.mean(axis=0)\n",
        "\n",
        "        ax_top = axes[0, i]\n",
        "        sample_slice = est[: max_samples, :].T\n",
        "        ax_top.plot(z, sample_slice, color=\"C0\", alpha=0.1)\n",
        "        ax_top.plot(z, mean_est, color=\"C0\", lw=2, label=\"ensemble mean\")\n",
        "        ax_top.plot(z, truth, color=\"k\", ls=\"--\", lw=2, label=\"truth\")\n",
        "        ax_top.set_ylim(bottom=-0.002)\n",
        "        ax_top.set_title(tomo[\"label\"])\n",
        "        if i == 0:\n",
        "            ax_top.set_ylabel(r\"$n(z)$\")\n",
        "        else:\n",
        "            ax_top.set_yticklabels([])\n",
        "\n",
        "        cov = np.cov(est, rowvar=False)\n",
        "        std = np.sqrt(np.clip(np.diag(cov), 1e-16, None))\n",
        "        corr = cov / np.outer(std, std)\n",
        "        corr = np.clip(corr, -1.0, 1.0)\n",
        "\n",
        "        ax_bottom = axes[1, i]\n",
        "        im = ax_bottom.imshow(\n",
        "            corr,\n",
        "            origin=\"lower\",\n",
        "            cmap=\"coolwarm\",\n",
        "            vmin=-1,\n",
        "            vmax=1,\n",
        "            extent=[z.min(), z.max(), z.min(), z.max()],\n",
        "            aspect=\"auto\",\n",
        "        )\n",
        "        ax_bottom.set_xlabel(\"z\")\n",
        "        if i == 0:\n",
        "            ax_bottom.set_ylabel(\"z\")\n",
        "        else:\n",
        "            ax_bottom.set_yticklabels([])\n",
        "\n",
        "    cax = fig.add_axes([0.92, 0.12, 0.02, 0.33])\n",
        "    fig.colorbar(im, cax=cax, label=\"corr\")\n",
        "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
        "    if handles:\n",
        "        fig.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(0.9, 0.96))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Set cross-correlation method\n",
        "crosscorrs_method = \"BinWise\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shifts Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Shifts prior model\n",
        "shifts_priors = nzp.PriorSacc(\n",
        "    sacc_catalog,\n",
        "    model=\"Shifts\",\n",
        "    compute_crosscorrs=crosscorrs_method,\n",
        ")\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "for model_obj in shifts_priors.model_objs.values():\n",
        "    shifts = np.asarray(model_obj.shifts)\n",
        "    if np.std(shifts) == 0:\n",
        "        model_obj.shifts = shifts + rng.normal(scale=1e-4, size=shifts.shape)\n",
        "        model_obj.params = model_obj._get_params()\n",
        "        model_obj.prior_mean = None\n",
        "        model_obj.prior_cov = None\n",
        "        model_obj.prior_chol = None\n",
        "\n",
        "shift_labels = [\n",
        "    r\"\\Delta z^{\\rm %s}\" % tomo[\"label\"].replace(\"_\", \" \")\n",
        "    for tomo in tomographic_bins\n",
        "]\n",
        "\n",
        "shift_plot = shifts_priors.plot_prior(\n",
        "    mode=\"1D\",\n",
        "    add_prior=True,\n",
        "    labels=shift_labels,\n",
        "    nx=5,\n",
        "    legend_ncol=2,\n",
        "    lws=2,\n",
        "    ls=[\"-\", \"--\"],\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get shift prior statistics and correlation matrix\n",
        "shift_mean, shift_cov, shift_chol = shifts_priors.get_prior()\n",
        "shift_err = np.sqrt(np.diag(shift_cov))\n",
        "shift_corr = shift_cov / np.outer(shift_err, shift_err)\n",
        "plt.imshow(shift_corr, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation matrix – Shifts\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save shift priors and generate shifted n(z) realizations\n",
        "dz_prior_dir = SAVE_ROOT / \"dz_priors\"\n",
        "dz_prior_dir.mkdir(parents=True, exist_ok=True)\n",
        "for tracer_name, model in shifts_priors.model_objs.items():\n",
        "    z = np.asarray(model.z, dtype=float)\n",
        "    nz_mean = np.asarray(model.nz_mean, dtype=float)\n",
        "    _, cov, chol = model.get_prior()\n",
        "    np.savez(\n",
        "        dz_prior_dir / f\"dz_{tracer_name}.npz\",\n",
        "        z=z,\n",
        "        dndz=nz_mean,\n",
        "        cov=cov,\n",
        "        chol=chol,\n",
        "    )\n",
        "\n",
        "shift_params = {name: np.asarray(model.params, dtype=float) for name, model in shifts_priors.model_objs.items()}\n",
        "np.savez(dz_prior_dir / \"dz_params.npz\", **shift_params)\n",
        "print(f\"Saved shift priors to {dz_prior_dir}\")\n",
        "\n",
        "# Generate shifted n(z) realizations\n",
        "shifted_nzs = {}\n",
        "first_tracer = list(sacc_catalog.tracers.keys())[0]\n",
        "n_realizations = shifts_priors.model_objs[first_tracer].params.shape[1]\n",
        "for i in range(n_realizations):\n",
        "    for tracer_name in sacc_catalog.tracers.keys():\n",
        "        model_obj = shifts_priors.model_objs[tracer_name]\n",
        "        z = model_obj.z\n",
        "        nz_mean = model_obj.nz_mean\n",
        "        shift_param = model_obj.params[:, i]\n",
        "        shifted_nz = nzp.shift_and_width_model(z, nz_mean, shift_param, 1.0)\n",
        "        if i == 0:\n",
        "            shifted_nzs[tracer_name] = [z, shifted_nz]\n",
        "        else:\n",
        "            shifted_nzs[tracer_name][1] = np.vstack([shifted_nzs[tracer_name][1], shifted_nz])\n",
        "\n",
        "shifted_tomos = tomos_from_draws(shifted_nzs, edges_lookup)\n",
        "make_nz_plots(shifted_tomos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shifts & Widths Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Shifts & Widths prior model\n",
        "snw_priors = nzp.PriorSacc(\n",
        "    sacc_catalog,\n",
        "    model=\"ShiftsWidths\",\n",
        "    compute_crosscorrs=crosscorrs_method,\n",
        ")\n",
        "\n",
        "for model_obj in snw_priors.model_objs.values():\n",
        "    params = np.asarray(model_obj.params)\n",
        "    # params shape (2, n_realizations)\n",
        "    if np.allclose(np.std(params, axis=1), 0):\n",
        "        jitter = np.zeros_like(params)\n",
        "        jitter[0] = rng.normal(scale=1e-4, size=params.shape[1])\n",
        "        jitter[1] = rng.normal(scale=1e-4, size=params.shape[1])\n",
        "        model_obj.params = params + jitter\n",
        "        model_obj.shifts = model_obj.params[0]\n",
        "        model_obj.widths = model_obj.params[1]\n",
        "        model_obj.prior_mean = None\n",
        "        model_obj.prior_cov = None\n",
        "        model_obj.prior_chol = None\n",
        "\n",
        "snw_labels = []\n",
        "for tomo in tomographic_bins:\n",
        "    base = tomo[\"label\"].replace(\"_\", \" \")\n",
        "    snw_labels.extend([\n",
        "        rf\"\\Delta z^{{\\rm {base}}}\",\n",
        "        rf\"w_z^{{\\rm {base}}}\",\n",
        "    ])\n",
        "\n",
        "snw_labels = np.array(snw_labels)\n",
        "snw_order = list(range(0, 2 * len(tomographic_bins), 2)) + list(\n",
        "    range(1, 2 * len(tomographic_bins), 2)\n",
        ")\n",
        "\n",
        "snw_plot = snw_priors.plot_prior(\n",
        "    order=np.array(snw_order),\n",
        "    labels=snw_labels,\n",
        "    mode=\"1D\",\n",
        "    nx=5,\n",
        "    legend_ncol=2,\n",
        "    lws=2,\n",
        "    ls=[\"-\", \"--\"],\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Shifts & Widths prior statistics and correlation matrix\n",
        "snw_mean, snw_cov, snw_chol = snw_priors.get_prior()\n",
        "snw_err = np.sqrt(np.diag(snw_cov))\n",
        "snw_corr = snw_cov / np.outer(snw_err, snw_err)\n",
        "plt.imshow(snw_corr, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation matrix – Shifts & Widths\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Save shift+width priors and generate realizations\n",
        "wzdz_dir = SAVE_ROOT / \"wzdz_priors\"\n",
        "wzdz_dir.mkdir(parents=True, exist_ok=True)\n",
        "for tracer_name, model in snw_priors.model_objs.items():\n",
        "    z = np.asarray(model.z, dtype=float)\n",
        "    nz_mean = np.asarray(model.nz_mean, dtype=float)\n",
        "    _, cov, chol = model.get_prior()\n",
        "    np.savez(\n",
        "        wzdz_dir / f\"wzdz_{tracer_name}.npz\",\n",
        "        z=z,\n",
        "        dndz=nz_mean,\n",
        "        cov=cov,\n",
        "        chol=chol,\n",
        "    )\n",
        "\n",
        "snw_params = {name: np.asarray(model.params, dtype=float) for name, model in snw_priors.model_objs.items()}\n",
        "np.savez(wzdz_dir / \"wzdz_params.npz\", **snw_params)\n",
        "print(f\"Saved shift+width priors to {wzdz_dir}\")\n",
        "\n",
        "# Generate shifted+width n(z) realizations\n",
        "snw_nzs = {}\n",
        "first_tracer = list(sacc_catalog.tracers.keys())[0]\n",
        "n_realizations = snw_priors.model_objs[first_tracer].params.shape[1]\n",
        "for i in range(n_realizations):\n",
        "    for tracer_name in sacc_catalog.tracers.keys():\n",
        "        model_obj = snw_priors.model_objs[tracer_name]\n",
        "        z = model_obj.z\n",
        "        nz_mean = model_obj.nz_mean\n",
        "        shift = model_obj.params[0, i]\n",
        "        width = model_obj.params[1, i]\n",
        "        shifted_nz = nzp.shift_and_width_model(z, nz_mean, shift, width)\n",
        "        if i == 0:\n",
        "            snw_nzs[tracer_name] = [z, shifted_nz]\n",
        "        else:\n",
        "            snw_nzs[tracer_name][1] = np.vstack([snw_nzs[tracer_name][1], shifted_nz])\n",
        "\n",
        "snw_tomos = tomos_from_draws(snw_nzs, edges_lookup)\n",
        "make_nz_plots(snw_tomos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian Process Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gaussian Process prior model\n",
        "gp_modes = 5\n",
        "gp_priors = nzp.PriorSacc(\n",
        "    sacc_catalog,\n",
        "    model=\"GP\",\n",
        "    compute_crosscorrs=crosscorrs_method,\n",
        "    nparams=gp_modes,\n",
        ")\n",
        "\n",
        "gp_mean, gp_cov, gp_chol = gp_priors.get_prior()\n",
        "gp_err = np.sqrt(np.diag(gp_cov))\n",
        "gp_corr = gp_cov / np.outer(gp_err, gp_err)\n",
        "plt.imshow(gp_corr, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation matrix – GP\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Save GP priors and generate realizations\n",
        "gp_dir = SAVE_ROOT / f\"gp_{gp_modes}_priors\"\n",
        "gp_dir.mkdir(parents=True, exist_ok=True)\n",
        "for tracer_name, model in gp_priors.model_objs.items():\n",
        "    z = np.asarray(model.z, dtype=float)\n",
        "    nz_mean = np.asarray(model.nz_mean, dtype=float)\n",
        "    _, _, chol = model.get_prior()\n",
        "    funcs = np.asarray(model.funcs, dtype=float)\n",
        "    np.savez(\n",
        "        gp_dir / f\"gp_{tracer_name}.npz\",\n",
        "        z=z,\n",
        "        dndz=nz_mean,\n",
        "        W=funcs,\n",
        "        chol=chol,\n",
        "    )\n",
        "\n",
        "gp_params = {name: np.asarray(model.params, dtype=float) for name, model in gp_priors.model_objs.items()}\n",
        "np.savez(gp_dir / f\"gp_params_{gp_modes}.npz\", **gp_params)\n",
        "print(f\"Saved GP priors to {gp_dir}\")\n",
        "\n",
        "# Generate GP n(z) realizations\n",
        "gp_nzs = {}\n",
        "for tracer_name in sacc_catalog.tracers.keys():\n",
        "    model_obj = gp_priors.model_objs[tracer_name]\n",
        "    z = model_obj.z\n",
        "    nz_mean = model_obj.nz_mean\n",
        "    funcs = model_obj.funcs\n",
        "    params = model_obj.params - np.mean(model_obj.params, axis=1, keepdims=True)\n",
        "    draws = [nzp.linear_model(nz_mean, funcs, params[:, i]) for i in range(params.shape[1])]\n",
        "    gp_nzs[tracer_name] = [z, np.asarray(draws, dtype=float)]\n",
        "\n",
        "gp_tomos = tomos_from_draws(gp_nzs, edges_lookup)\n",
        "make_nz_plots(gp_tomos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PCA prior model\n",
        "pca_modes = 5\n",
        "pca_priors = nzp.PriorSacc(\n",
        "    sacc_catalog,\n",
        "    model=\"PCA\",\n",
        "    compute_crosscorrs=crosscorrs_method,\n",
        "    nparams=pca_modes,\n",
        ")\n",
        "\n",
        "pca_mean, pca_cov, pca_chol = pca_priors.get_prior()\n",
        "pca_err = np.sqrt(np.diag(pca_cov))\n",
        "pca_corr = pca_cov / np.outer(pca_err, pca_err)\n",
        "plt.imshow(np.sqrt(np.abs(pca_corr)), cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation matrix – PCA (|corr|^{1/2})\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Save PCA priors and generate realizations\n",
        "pca_dir = SAVE_ROOT / f\"pca_{pca_modes}_priors\"\n",
        "pca_dir.mkdir(parents=True, exist_ok=True)\n",
        "for tracer_name, model in pca_priors.model_objs.items():\n",
        "    z = np.asarray(model.z, dtype=float)\n",
        "    nz_mean = np.asarray(model.nz_mean, dtype=float)\n",
        "    _, _, chol = model.get_prior()\n",
        "    funcs = np.asarray(model.funcs, dtype=float)\n",
        "    np.savez(\n",
        "        pca_dir / f\"pca_{tracer_name}.npz\",\n",
        "        z=z,\n",
        "        dndz=nz_mean,\n",
        "        W=funcs,\n",
        "        chol=chol,\n",
        "    )\n",
        "\n",
        "pca_params = {name: np.asarray(model.params, dtype=float) for name, model in pca_priors.model_objs.items()}\n",
        "np.savez(pca_dir / f\"pca_params_{pca_modes}.npz\", **pca_params)\n",
        "print(f\"Saved PCA priors to {pca_dir}\")\n",
        "\n",
        "# Generate PCA n(z) realizations\n",
        "pca_nzs = {}\n",
        "for tracer_name in sacc_catalog.tracers.keys():\n",
        "    model_obj = pca_priors.model_objs[tracer_name]\n",
        "    z = model_obj.z\n",
        "    nz_mean = model_obj.nz_mean\n",
        "    funcs = model_obj.funcs\n",
        "    params = model_obj.params\n",
        "    draws = [nzp.linear_model(nz_mean, funcs, params[:, i]) for i in range(params.shape[1])]\n",
        "    pca_nzs[tracer_name] = [z, np.asarray(draws, dtype=float)]\n",
        "\n",
        "pca_tomos = tomos_from_draws(pca_nzs, edges_lookup)\n",
        "make_nz_plots(pca_tomos)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rail",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
